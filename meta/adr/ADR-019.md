---
title: "ADR-019: Monitoring, Metrics, and Observability Strategy"
status: "Proposed"
date: "2025-01-27"
tags:
  - "monitoring"
  - "observability"
  - "metrics"
  - "alerting"
---

## Context

* **Problem:** The Hoopstat Haus data platform requires comprehensive monitoring and observability to ensure system reliability, performance optimization, and rapid incident response. With data pipelines, APIs, and web services, we need visibility into system health, data quality, and user experience.
* **Constraints:** Must integrate with AWS services, provide real-time alerting for critical issues, support both infrastructure and application-level monitoring, enable data pipeline observability, and maintain cost-effectiveness for a solo developer project.

## Decision

We will implement a **comprehensive observability strategy using CloudWatch for metrics and logs, with Grafana for visualization and PagerDuty for critical alerting**, following the **three pillars of observability: logs, metrics, and traces**.

## Considered Options

1. **CloudWatch + Grafana + PagerDuty (The Chosen One):** AWS-native monitoring with enhanced visualization and alerting.
   * *Pros:* Deep AWS integration, cost-effective for AWS workloads, powerful custom metrics, Grafana provides superior visualization, PagerDuty for reliable critical alerting, no additional infrastructure to manage
   * *Cons:* Grafana adds complexity, potential vendor lock-in to AWS monitoring, limited distributed tracing compared to specialized tools

2. **Prometheus + Grafana + AlertManager:** Open-source monitoring stack.
   * *Pros:* Full control over monitoring infrastructure, excellent metrics collection, powerful querying with PromQL, vendor-neutral
   * *Cons:* Additional infrastructure to manage, higher operational overhead, requires expertise in Prometheus operations, cost for hosting monitoring infrastructure

3. **DataDog:** Comprehensive SaaS monitoring platform.
   * *Pros:* Excellent user experience, comprehensive features, minimal setup, strong alerting capabilities
   * *Cons:* Expensive for growing data volumes, vendor lock-in, less customization control

4. **AWS CloudWatch Only:** Use only native AWS monitoring.
   * *Pros:* Simple setup, deep AWS integration, no additional tools, cost-effective
   * *Cons:* Limited visualization capabilities, basic alerting options, poor user experience for complex dashboards

## Monitoring Architecture

### Three Pillars Implementation

#### 1. Logs (Already Covered in ADR-015)
- **Structured JSON logging:** All applications use consistent JSON format
- **Centralized collection:** CloudWatch Logs for all application and infrastructure logs
- **Log aggregation:** CloudWatch Insights for searching and analysis
- **Retention policy:** 30 days for debug logs, 1 year for audit logs

#### 2. Metrics
```python
# Application metrics structure
{
    "namespace": "HoopstatHaus",
    "metrics": [
        {
            "metric_name": "api_request_duration",
            "dimensions": {"service": "stats-api", "endpoint": "/games"},
            "value": 245.67,
            "unit": "Milliseconds"
        },
        {
            "metric_name": "data_pipeline_records_processed", 
            "dimensions": {"pipeline": "daily_stats", "stage": "silver"},
            "value": 15420,
            "unit": "Count"
        }
    ]
}
```

#### 3. Traces (Future Implementation)
- **AWS X-Ray:** For distributed tracing across services
- **Trace correlation:** Link traces to logs via correlation IDs
- **Performance analysis:** Request flow and bottleneck identification

## Monitoring Categories

### Infrastructure Monitoring
**AWS CloudWatch native metrics:**
- **EC2/ECS:** CPU, memory, disk, network utilization
- **RDS:** Database performance, connections, slow queries
- **S3:** Bucket size, request metrics, data transfer
- **Lambda:** Duration, errors, throttles, cold starts
- **Load Balancers:** Request count, latency, error rates

### Application Monitoring
**Custom application metrics:**
```python
# Example metrics in Python applications
import boto3

cloudwatch = boto3.client('cloudwatch')

# API performance metrics
cloudwatch.put_metric_data(
    Namespace='HoopstatHaus/API',
    MetricData=[
        {
            'MetricName': 'ResponseTime',
            'Dimensions': [
                {'Name': 'Endpoint', 'Value': '/api/v1/games'},
                {'Name': 'Method', 'Value': 'GET'}
            ],
            'Value': response_time_ms,
            'Unit': 'Milliseconds'
        }
    ]
)

# Data quality metrics
cloudwatch.put_metric_data(
    Namespace='HoopstatHaus/DataQuality',
    MetricData=[
        {
            'MetricName': 'RecordsWithErrors',
            'Dimensions': [
                {'Name': 'Pipeline', 'Value': 'daily_stats'},
                {'Name': 'DataSource', 'Value': 'nba_api'}
            ],
            'Value': error_count,
            'Unit': 'Count'
        }
    ]
)
```

### Data Pipeline Monitoring
**Critical data pipeline metrics:**
- **Execution metrics:** Duration, success/failure rates, records processed
- **Data quality:** Null rates, schema violations, data freshness
- **Cost tracking:** Processing costs, storage costs per pipeline
- **SLA compliance:** Pipeline completion times vs. targets

### Business Metrics
**Key business indicators:**
- **API usage:** Requests per day, unique users, popular endpoints
- **Data freshness:** Time since last update for key datasets
- **System availability:** Uptime percentage, error rates
- **Cost efficiency:** Cost per API request, storage cost trends

## Alerting Strategy

### Alert Severity Levels
```yaml
Critical (PagerDuty + SMS):
  - API error rate > 5%
  - Database unavailable
  - Data pipeline failure > 2 hours
  - Storage costs > $200/day

Warning (Email):
  - API latency > 2 seconds (95th percentile)
  - Data pipeline delay > 1 hour
  - Disk usage > 80%
  - Unusual traffic patterns

Info (Dashboard only):
  - Cost trends
  - Performance trends
  - Usage statistics
```

### Alert Configuration
```python
# CloudWatch Alarm example
{
    "AlarmName": "HighAPIErrorRate",
    "MetricName": "ErrorRate",
    "Namespace": "HoopstatHaus/API",
    "Statistic": "Average",
    "Period": 300,
    "EvaluationPeriods": 2,
    "Threshold": 5.0,
    "ComparisonOperator": "GreaterThanThreshold",
    "AlarmActions": ["arn:aws:sns:us-east-1:account:critical-alerts"]
}
```

## Dashboard Strategy

### Grafana Dashboards
1. **System Overview:** High-level health of all services
2. **API Performance:** Request rates, latency, error rates by endpoint
3. **Data Pipeline Status:** Pipeline execution, data quality, costs
4. **Infrastructure Health:** AWS resource utilization and costs
5. **Business Metrics:** Usage trends, popular features, user engagement

### CloudWatch Dashboards
- **Real-time operational:** Quick system health check
- **Cost monitoring:** Daily/weekly cost trends
- **Incident response:** Focused views for troubleshooting

## Implementation Plan

### Phase 1: Foundation (Immediate)
- Set up CloudWatch custom metrics in application template
- Create basic CloudWatch dashboards
- Implement critical alerting (error rates, downtime)

### Phase 2: Enhanced Monitoring (Month 2)
- Deploy Grafana instance for advanced visualization
- Create comprehensive dashboards
- Set up PagerDuty integration for critical alerts

### Phase 3: Advanced Observability (Month 3)
- Implement AWS X-Ray tracing
- Add data quality monitoring
- Business metrics dashboard

## Consequences

* **Positive:** Proactive issue detection through comprehensive monitoring, rapid incident response with proper alerting, data-driven optimization through metrics visibility, cost awareness through monitoring dashboard, improved system reliability through observability, foundation for SLA management and capacity planning.
* **Negative:** Additional complexity in application instrumentation, ongoing cost for monitoring tools and storage, requires discipline to maintain dashboards and alerts, potential alert fatigue if not properly tuned.
* **Future Implications:** All applications must include monitoring instrumentation from day one, incident response procedures will be data-driven, performance optimization will be based on actual metrics, cost management will be proactive rather than reactive, system reliability improvements will be measurable and tracked.