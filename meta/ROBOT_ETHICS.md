# Robot Ethics & Data Sourcing Policy

## 1. Preamble

This document outlines the ethical principles that govern all data gathering, scraping, and API interaction conducted by the `hoopstat-haus` project. Our goal is to build a useful and interesting dataset while acting as good citizens of the digital world. Adherence to these principles is mandatory for all code, whether generated by AI or written by humans. Our project's long-term viability depends on the trust and respect we show to the data sources we rely on.

## 2. Core Principles

### 2.1. We Identify Ourselves

All automated network requests made by this project **must** use a descriptive `User-Agent` string that clearly identifies the project and provides a way to get more information. This promotes transparency and allows server administrators to understand the source of the traffic they receive.

* **Example `User-Agent`:** `hoopstat-haus/0.1 (AI-driven research project; +https://github.com/efischer19/hoopstat-haus)`

### 2.2. We Respect Server Load

We recognize that our operations consume the resources of others. To minimize our impact, we **must** operate with restraint.

* **Rate Limiting:** The system will strictly adhere to any published rate limits for an API.
* **Polite Throttling:** For sources without explicit rate limits, the system will operate at a respectful pace, introducing delays between requests to avoid causing a "denial of service" effect.
* **Error Handling:** The system **must** gracefully handle HTTP `429 (Too Many Requests)` and `5xx` server errors by implementing an exponential backoff strategy, waiting progressively longer before retrying a failed request.

### 2.3. We Obey `robots.txt`

Before performing any web scraping on a domain, the system **must** first fetch and parse the `robots.txt` file. All rules specified within, particularly `Disallow` directives, will be unconditionally respected. If a resource is disallowed, it will not be accessed.

### 2.4. We Cache Aggressively

To avoid requesting the same information repeatedly, the system **will** employ an aggressive caching strategy. This reduces load on our data sources and improves the performance and efficiency of our own project.

### 2.5. We Adhere to Terms of Service

For any third-party API or service, the project will operate strictly within the bounds of the provider's Terms of Service (ToS). This includes, but is not limited to, policies on data usage, authentication, and attribution. A review of the ToS is a prerequisite for integrating any new data source.

### 2.6. We Attribute Our Sources

Just as in academia or journalism, giving credit is a fundamental ethical requirement. The project **will** maintain a clear and accessible record of where its data originates. Where data is presented, attribution to the original source will be provided in a manner consistent with their licensing or terms.
